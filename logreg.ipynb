{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dc11eb19",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from torch import nn, optim\n",
    "from sklearn.model_selection import KFold\n",
    "import pandas as pd\n",
    "from datetime import timedelta\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "daf7b30e",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         date    close\n",
      "0  2020-04-09  2789.82\n",
      "1  2020-04-13  2761.63\n",
      "2  2020-04-14  2846.06\n",
      "3  2020-04-15  2783.36\n",
      "4  2020-04-16  2799.55\n",
      "..        ...      ...\n",
      "58 2020-07-02  3130.01\n",
      "59 2020-07-06  3179.72\n",
      "60 2020-07-07  3145.32\n",
      "61 2020-07-08  3169.94\n",
      "62 2020-07-09  3152.05\n",
      "\n",
      "[63 rows x 2 columns]\n",
      "         date  close     Today  Yesterday  twoDaysPrior  threeDaysPrior  \\\n",
      "0  2020-04-09      0  1.193676   1.181529      1.149408        1.180180   \n",
      "1  2020-04-13      0  1.193955   1.143198      1.110070        1.145299   \n",
      "2  2020-04-14      1  1.097561   1.193955      1.143198        1.110070   \n",
      "3  2020-04-15      0  1.102343   1.097561      1.193955        1.143198   \n",
      "4  2020-04-16      1  1.162500   1.102343      1.097561        1.193955   \n",
      "5  2020-04-17      1  1.151930   1.162500      1.102343        1.097561   \n",
      "6  2020-04-20      0  1.169811   1.244332      1.100756        1.151930   \n",
      "7  2020-04-21      0  1.131617   1.169811      1.244332        1.100756   \n",
      "8  2020-04-22      1  1.147368   1.131617      1.169811        1.244332   \n",
      "9  2020-04-23      0  1.141097   1.147368      1.131617        1.169811   \n",
      "10 2020-05-28      0  1.152310   1.093904      1.181818        0.996226   \n",
      "11 2020-05-29      1  1.237956   1.152310      1.093904        1.181818   \n",
      "12 2020-06-01      1  1.321244   1.266026      1.200748        1.237956   \n",
      "13 2020-06-02      1  1.247619   1.321244      1.266026        1.200748   \n",
      "14 2020-06-03      1  1.242350   1.247619      1.321244        1.266026   \n",
      "15 2020-06-04      0  1.207547   1.242350      1.247619        1.321244   \n",
      "16 2020-06-05      1  1.071517   1.207547      1.242350        1.247619   \n",
      "17 2020-06-15      1  1.290030   1.156863      1.157761        1.197031   \n",
      "18 2020-06-16      1  1.223005   1.290030      1.156863        1.157761   \n",
      "19 2020-06-17      0  1.247017   1.223005      1.290030        1.156863   \n",
      "20 2020-06-18      1  1.153465   1.247017      1.223005        1.290030   \n",
      "21 2020-06-19      0  1.163001   1.153465      1.247017        1.223005   \n",
      "22 2020-06-22      1  1.115108   1.130435      1.082143        1.163001   \n",
      "23 2020-06-23      1  1.197647   1.115108      1.130435        1.082143   \n",
      "24 2020-06-24      0  1.296386   1.197647      1.115108        1.130435   \n",
      "25 2020-06-25      1  1.253886   1.296386      1.197647        1.115108   \n",
      "26 2020-06-26      0  1.156479   1.253886      1.296386        1.197647   \n",
      "27 2020-06-29      1  1.367647   1.248031      1.205195        1.156479   \n",
      "28 2020-06-30      1  1.310935   1.367647      1.248031        1.205195   \n",
      "29 2020-07-01      1  1.215496   1.310935      1.367647        1.248031   \n",
      "30 2020-07-02      1  1.199561   1.215496      1.310935        1.367647   \n",
      "31 2020-07-06      1  1.287879   1.277657      1.315496        1.190639   \n",
      "32 2020-07-07      0  1.297462   1.287879      1.277657        1.315496   \n",
      "33 2020-07-08      1  1.217054   1.297462      1.287879        1.277657   \n",
      "34 2020-07-09      0  1.322148   1.217054      1.297462        1.287879   \n",
      "\n",
      "    fourDaysPrior  fiveDaysPrior  sixDaysPrior  \n",
      "0        1.097222       1.132353      1.179648  \n",
      "1        1.193676       1.181529      1.149408  \n",
      "2        1.145299       1.193676      1.181529  \n",
      "3        1.110070       1.145299      1.193676  \n",
      "4        1.143198       1.110070      1.145299  \n",
      "5        1.193955       1.143198      1.110070  \n",
      "6        1.162500       1.102343      1.097561  \n",
      "7        1.151930       1.162500      1.102343  \n",
      "8        1.100756       1.151930      1.162500  \n",
      "9        1.244332       1.100756      1.151930  \n",
      "10       1.173585       1.134021      1.136302  \n",
      "11       0.996226       1.173585      1.134021  \n",
      "12       1.152310       1.093904      1.181818  \n",
      "13       1.237956       1.152310      1.093904  \n",
      "14       1.200748       1.237956      1.152310  \n",
      "15       1.266026       1.200748      1.237956  \n",
      "16       1.321244       1.266026      1.200748  \n",
      "17       1.203193       1.193732      1.178295  \n",
      "18       1.197031       1.203193      1.193732  \n",
      "19       1.157761       1.197031      1.203193  \n",
      "20       1.156863       1.157761      1.197031  \n",
      "21       1.290030       1.156863      1.157761  \n",
      "22       1.153465       1.247017      1.223005  \n",
      "23       1.163001       1.153465      1.247017  \n",
      "24       1.082143       1.163001      1.153465  \n",
      "25       1.130435       1.082143      1.163001  \n",
      "26       1.115108       1.130435      1.082143  \n",
      "27       1.253886       1.296386      1.197647  \n",
      "28       1.156479       1.253886      1.296386  \n",
      "29       1.205195       1.156479      1.253886  \n",
      "30       1.248031       1.205195      1.156479  \n",
      "31       1.199561       1.215496      1.310935  \n",
      "32       1.190639       1.199561      1.215496  \n",
      "33       1.315496       1.190639      1.199561  \n",
      "34       1.277657       1.315496      1.190639  \n"
     ]
    }
   ],
   "source": [
    "#Make dataframe for CNN training\n",
    "dates = pd.read_pickle(\"by_date.pkl\")\n",
    "dates = pd.DataFrame(dates)\n",
    "sp500 = pd.read_pickle(\"sp500.pkl\")\n",
    "sp500 = pd.DataFrame(sp500)\n",
    "sp500 = sp500.tail(63).copy()\n",
    "sp500 = sp500.iloc[::-1]\n",
    "sp500.reset_index(drop=True, inplace = True)\n",
    "sp500.drop(\"open\", axis=1, inplace = True)\n",
    "print(sp500)\n",
    "sp500['close'] = sp500['close'].diff().apply(lambda x: 1 if x >= 0 else 0)\n",
    "# sp500 = sp500.iloc[1: , :]\n",
    "# print(sp500)\n",
    "\n",
    "dates = dates.T \n",
    "dates.columns.name = None\n",
    "dates.columns = dates.columns.map(str)\n",
    "dates.reset_index(drop=True, inplace = True)\n",
    "# display(sp500.to_string())\n",
    "# display(dates.to_string())\n",
    "sp500.drop(sp500.index[[10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,\n",
    "                       40,41,42,43,44]], axis=0, inplace=True)\n",
    "sp500.reset_index(drop=True, inplace = True)\n",
    "sp500 = sp500.reindex(columns = sp500.columns.tolist() + ['Today','Yesterday', 'twoDaysPrior',\n",
    "                                                        'threeDaysPrior', 'fourDaysPrior', 'fiveDaysPrior',\n",
    "                                                        'sixDaysPrior'])\n",
    "for row in range(35):\n",
    "    sp500.at[row, 'sixDaysPrior'] = dates.at[0,sp500.at[row,'date'].strftime(\"%Y-%m-%d\")]\n",
    "    sp500.at[row, 'fiveDaysPrior'] = dates.at[0,(sp500.at[row,'date']+timedelta(days = 1)).strftime(\"%Y-%m-%d\")]\n",
    "    sp500.at[row, 'fourDaysPrior'] = dates.at[0,(sp500.at[row,'date']+timedelta(days = 2)).strftime(\"%Y-%m-%d\")]\n",
    "    sp500.at[row, 'threeDaysPrior'] = dates.at[0,(sp500.at[row,'date']+timedelta(days = 3)).strftime(\"%Y-%m-%d\")]\n",
    "    sp500.at[row, 'twoDaysPrior'] = dates.at[0,(sp500.at[row,'date']+timedelta(days = 4)).strftime(\"%Y-%m-%d\")]\n",
    "    sp500.at[row, 'Yesterday'] = dates.at[0,(sp500.at[row,'date']+timedelta(days = 5)).strftime(\"%Y-%m-%d\")]\n",
    "    sp500.at[row, 'Today'] = dates.at[0,(sp500.at[row,'date']+timedelta(days = 6)).strftime(\"%Y-%m-%d\")]\n",
    "sp500 = sp500.reset_index().drop(\"index\", axis = 1)\n",
    "print(sp500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "55077b51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.625\n"
     ]
    }
   ],
   "source": [
    "cv = KFold(n_splits=20, random_state=1, shuffle=True)\n",
    "model = LogisticRegression()\n",
    "all_X = sp500.drop([\"date\", \"close\"], axis = 1)\n",
    "all_y = sp500[\"close\"]\n",
    "scores = cross_val_score(model, all_X, all_y, scoring='accuracy', cv=cv, n_jobs=-1)\n",
    "print(np.mean(scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7538121",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Failed attempts:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "32625e28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class Logistic_Regression(torch.nn.Module):\n",
    "#     def __init__(self):\n",
    "#         super(Logistic_Regression,self).__init__()\n",
    "#         self.layer1=torch.nn.Linear(7,32)\n",
    "#         self.layer2=torch.nn.Linear(32,1)\n",
    "#     def forward(self,x):\n",
    "#         y_predicted=self.layer1(x)\n",
    "#         y_predicted=torch.sigmoid(self.layer2(y_predicted))\n",
    "#         return y_predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "17e211a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 10 ,loss= 0.24235625565052032\n",
      "epoch: 20 ,loss= 0.2375141680240631\n",
      "epoch: 30 ,loss= 0.2332400232553482\n",
      "epoch: 40 ,loss= 0.22945845127105713\n",
      "epoch: 50 ,loss= 0.22610056400299072\n",
      "epoch: 60 ,loss= 0.2231055647134781\n",
      "epoch: 70 ,loss= 0.22042086720466614\n",
      "epoch: 80 ,loss= 0.21800199151039124\n",
      "epoch: 90 ,loss= 0.21581126749515533\n",
      "epoch: 100 ,loss= 0.21381716430187225\n"
     ]
    }
   ],
   "source": [
    "# model=Logistic_Regression()\n",
    "\n",
    "# criterion=torch.nn.MSELoss()\n",
    "# optimizer=torch.optim.SGD(model.parameters(),lr=0.01)\n",
    "\n",
    "# number_of_epochs=100\n",
    "# for epoch in range(number_of_epochs):\n",
    "#     y_prediction=model(x_train)\n",
    "# #     print(y_prediction)\n",
    "# #     print(y_train)\n",
    "#     loss=criterion(y_prediction,y_train)\n",
    "# #     print(loss)\n",
    "#     loss.backward()\n",
    "#     optimizer.step()\n",
    "#     if (epoch+1)%10 == 0:\n",
    "#         print('epoch:', epoch+1,',loss=',loss.item())\n",
    "#     optimizer.zero_grad()\n",
    "# #     break;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4b5ea518",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.625\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "b673848e",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    index       date    close     Today  Yesterday  twoDaysPrior  \\\n",
      "12     12 2020-06-01  3055.73  1.321244   1.266026      1.200748   \n",
      "8       8 2020-04-22  2799.31  1.147368   1.131617      1.169811   \n",
      "16     16 2020-06-05  3193.93  1.071517   1.207547      1.242350   \n",
      "9       9 2020-04-23  2797.80  1.141097   1.147368      1.131617   \n",
      "33     33 2020-07-08  3169.94  1.217054   1.297462      1.287879   \n",
      "0       0 2020-04-09  2789.82  1.193676   1.181529      1.149408   \n",
      "4       4 2020-04-16  2799.55  1.162500   1.102343      1.097561   \n",
      "17     17 2020-06-15  3066.59  1.290030   1.156863      1.157761   \n",
      "27     27 2020-06-29  3053.24  1.367647   1.248031      1.205195   \n",
      "5       5 2020-04-17  2874.56  1.151930   1.162500      1.102343   \n",
      "11     11 2020-05-29  3044.31  1.237956   1.152310      1.093904   \n",
      "1       1 2020-04-13  2761.63  1.193955   1.143198      1.110070   \n",
      "2       2 2020-04-14  2846.06  1.097561   1.193955      1.143198   \n",
      "32     32 2020-07-07  3145.32  1.297462   1.287879      1.277657   \n",
      "3       3 2020-04-15  2783.36  1.102343   1.097561      1.193955   \n",
      "30     30 2020-07-02  3130.01  1.199561   1.215496      1.310935   \n",
      "23     23 2020-06-23  3131.29  1.197647   1.115108      1.130435   \n",
      "31     31 2020-07-06  3179.72  1.287879   1.277657      1.315496   \n",
      "10     10 2020-05-28  3029.73  1.152310   1.093904      1.181818   \n",
      "22     22 2020-06-22  3117.86  1.115108   1.130435      1.082143   \n",
      "18     18 2020-06-16  3124.74  1.223005   1.290030      1.156863   \n",
      "25     25 2020-06-25  3083.76  1.253886   1.296386      1.197647   \n",
      "6       6 2020-04-20  2823.16  1.169811   1.244332      1.100756   \n",
      "20     20 2020-06-18  3115.34  1.153465   1.247017      1.223005   \n",
      "34     34 2020-07-09  3152.05  1.322148   1.217054      1.297462   \n",
      "7       7 2020-04-21  2736.56  1.131617   1.169811      1.244332   \n",
      "14     14 2020-06-03  3122.87  1.242350   1.247619      1.321244   \n",
      "28     28 2020-06-30  3100.29  1.310935   1.367647      1.248031   \n",
      "\n",
      "    threeDaysPrior  fourDaysPrior  fiveDaysPrior  sixDaysPrior  \n",
      "12        1.237956       1.152310       1.093904      1.181818  \n",
      "8         1.244332       1.100756       1.151930      1.162500  \n",
      "16        1.247619       1.321244       1.266026      1.200748  \n",
      "9         1.169811       1.244332       1.100756      1.151930  \n",
      "33        1.277657       1.315496       1.190639      1.199561  \n",
      "0         1.180180       1.097222       1.132353      1.179648  \n",
      "4         1.193955       1.143198       1.110070      1.145299  \n",
      "17        1.197031       1.203193       1.193732      1.178295  \n",
      "27        1.156479       1.253886       1.296386      1.197647  \n",
      "5         1.097561       1.193955       1.143198      1.110070  \n",
      "11        1.181818       0.996226       1.173585      1.134021  \n",
      "1         1.145299       1.193676       1.181529      1.149408  \n",
      "2         1.110070       1.145299       1.193676      1.181529  \n",
      "32        1.315496       1.190639       1.199561      1.215496  \n",
      "3         1.143198       1.110070       1.145299      1.193676  \n",
      "30        1.367647       1.248031       1.205195      1.156479  \n",
      "23        1.082143       1.163001       1.153465      1.247017  \n",
      "31        1.190639       1.199561       1.215496      1.310935  \n",
      "10        0.996226       1.173585       1.134021      1.136302  \n",
      "22        1.163001       1.153465       1.247017      1.223005  \n",
      "18        1.157761       1.197031       1.203193      1.193732  \n",
      "25        1.115108       1.130435       1.082143      1.163001  \n",
      "6         1.151930       1.162500       1.102343      1.097561  \n",
      "20        1.290030       1.156863       1.157761      1.197031  \n",
      "34        1.287879       1.277657       1.315496      1.190639  \n",
      "7         1.100756       1.151930       1.162500      1.102343  \n",
      "14        1.266026       1.200748       1.237956      1.152310  \n",
      "28        1.205195       1.156479       1.253886      1.296386  \n"
     ]
    }
   ],
   "source": [
    "# print(sp500)\n",
    "# trainset, testset = train_test_split(sp500, test_size=0.2, random_state = 42)\n",
    "# trainloader = torch.utils.data.DataLoader(trainset, batch_size=16, shuffle=True)\n",
    "# testloader = torch.utils.data.DataLoader(testset, batch_size=16, shuffle=True)\n",
    "# print(trainset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b86fe3a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "042b675e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class CNN(nn.Module):\n",
    "#     def __init__(self, in_layer, out_layer, kernel_size, stride):\n",
    "#         super(CNN, self).__init__()\n",
    "#         self.conv1 = nn.Conv1d(in_layer, out_layer, kernel_size=kernel_size, stride=stride, padding = 3, bias=True)\n",
    "#         self.bn = nn.BatchNorm1d(out_layer)\n",
    "#         self.relu = nn.ReLU()\n",
    "    \n",
    "#     def forward(self,x):\n",
    "#         x = self.conv1(x)\n",
    "#         x = self.bn(x)\n",
    "#         out = self.relu(x)\n",
    "        \n",
    "#         return out       \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "7f183f48",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         date    close     Today  Yesterday  twoDaysPrior  threeDaysPrior  \\\n",
      "12 2020-06-01  3055.73  1.321244   1.266026      1.200748        1.237956   \n",
      "8  2020-04-22  2799.31  1.147368   1.131617      1.169811        1.244332   \n",
      "16 2020-06-05  3193.93  1.071517   1.207547      1.242350        1.247619   \n",
      "9  2020-04-23  2797.80  1.141097   1.147368      1.131617        1.169811   \n",
      "33 2020-07-08  3169.94  1.217054   1.297462      1.287879        1.277657   \n",
      "\n",
      "    fourDaysPrior  fiveDaysPrior  sixDaysPrior  \n",
      "12       1.152310       1.093904      1.181818  \n",
      "8        1.100756       1.151930      1.162500  \n",
      "16       1.321244       1.266026      1.200748  \n",
      "9        1.244332       1.100756      1.151930  \n",
      "33       1.315496       1.190639      1.199561  \n"
     ]
    }
   ],
   "source": [
    "# print(trainset.head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "50fe4302",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "expected np.ndarray (got DataFrame)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-111-d041e56236fa>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0mindices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpermutation\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0mbatch_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_y\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_x\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_y\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m         \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_x\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_y\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m         \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0mopt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: expected np.ndarray (got DataFrame)"
     ]
    }
   ],
   "source": [
    "# opt = torch.optim.Adam(model.parameters(),lr=learning_rate)\n",
    "# training_epochs = 10\n",
    "# criterion = nn.CrossEntropyLoss()\n",
    "# train_y = trainset[\"close\"]\n",
    "# train_x = trainset.drop([\"date\", \"close\", \"index\"], axis = 1)\n",
    "# batch_size = 5\n",
    "# # print(train_x['close'])\n",
    "\n",
    "# for e in range(training_epochs):\n",
    "#     train_losses = []\n",
    "#     permutation = torch.randperm(trainset.shape[0])\n",
    "# #     print(permutation)\n",
    "#     for i in range(0,trainset.shape[0], batch_size):\n",
    "#         indices = permutation[i:i+batch_size]\n",
    "#         batch_x, batch_y = train_x.iloc[indices], train_y.iloc[indices]\n",
    "#         inputs, targets = torch.from_numpy(batch_x), torch.from_numpy(batch_y)\n",
    "#         inputs, targets = inputs.cuda(), targets.cuda()\n",
    "#         opt.zero_grad()   \n",
    "#         output = model(inputs, batch_size)\n",
    "\n",
    "#         loss = criterion(output, targets.long())\n",
    "#         train_losses.append(loss.item())\n",
    "#         loss.backward()\n",
    "#         opt.step()\n",
    "#     val_losses = []\n",
    "#     accuracy=0\n",
    "#     f1score=0\n",
    "#     print(\"Epoch: {}/{}...\".format(e+1, training_epochs),\n",
    "#               \"Train Loss: {:.4f}...\".format(np.mean(train_losses)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15cf3c64",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# trainset, testset = train_test_split(sp500, test_size=0.2, random_state = 42)\n",
    "# x_train = trainset.drop([\"date\", \"close\"], axis = 1)\n",
    "# y_train = trainset[\"close\"]\n",
    "# x_test = testset.drop([\"date\", \"close\"], axis = 1)\n",
    "# y_test = testset[\"close\"]\n",
    "\n",
    "# scaler=sklearn.preprocessing.StandardScaler()\n",
    "# x_train=scaler.fit_transform(x_train)\n",
    "# x_test=scaler.transform(x_test)\n",
    "\n",
    "# x_train=torch.from_numpy(x_train.astype(np.float32))\n",
    "# x_test=torch.from_numpy(x_test.astype(np.float32))\n",
    "# y_train=torch.from_numpy(y_train.to_numpy().astype(np.float32))\n",
    "# y_test=torch.from_numpy(y_test.to_numpy().astype(np.float32))\n",
    "\n",
    "# y_train=y_train.view(y_train.shape[0],1)\n",
    "# y_test=y_test.view(y_test.shape[0],1)\n",
    "# # train_x = trainset.drop([\"date\", \"close\", \"index\"], axis = 1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p36",
   "language": "python",
   "name": "conda_pytorch_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
